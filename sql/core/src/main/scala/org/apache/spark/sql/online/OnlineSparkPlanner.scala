package org.apache.spark.sql.online

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.catalyst.analysis.{HiveTypeCoercion, UnresolvedException}
import org.apache.spark.sql.catalyst.errors.TreeNodeException
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.rules.Rule
import org.apache.spark.sql.catalyst.trees
import org.apache.spark.sql.catalyst.types._
import org.apache.spark.sql.execution._
import org.apache.spark.sql.online.OnlinePlannerUtil._
import org.apache.spark.sql.parquet.ParquetTableScan

import scala.collection.mutable
import scala.collection.mutable.ArrayBuffer

class OnlineSparkPlanner(sampledRelations: Set[String], numBatches: Int)(sqlContext: SQLContext) {

  val rules =
    SafetyCheck ::
    InsertSampledRelations(sampledRelations, numBatches) ::
    Resample ::
    ImplementResample ::
    PropagateBootstrap ::
    IdentifyUncertainTuples ::
    PruneFilterProjectColumns ::
    ConsolidateBootstrap ::
    IdentifyLazyEvaluates ::
    EmbedLineage ::
    ImplementGuard ::
    ImplementVectorize ::
    ImplementAggregate ::
    CleanupAnalysisExpressions ::
    Nil

  def apply(plan: SparkPlan): SparkPlan = rules.foldLeft(plan)((p, rule) => rule(p))
}

object SafetyCheck extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transform {
    case leaf: LeafNode => leaf
    case filter: Filter => filter
    case project: Project => project
    case exchange: Exchange => exchange
    case aggregate: Aggregate => aggregate
    case join: BroadcastHashJoin => join
    case join: ShuffledHashJoin => join
    case join: LeftSemiJoinHash => join
    case _ => ???
  } transformAllExpressions {
    case alias: Alias =>
      alias transformChildrenDown {
        case Alias(child, _) => child
      }
  }
}

// TODO
case class InsertSampledRelations(sampledRelations: Set[String], numBatches: Int) extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case tableScan: ParquetTableScan if sampledRelations.exists(tableScan.relation.path.endsWith) =>
      require(tableScan.relation.partitioningAttributes == Nil,
        "We do not support sampling partitioned input relations for now.")
      SampledRelation(0, numBatches, tableScan)

    case leaf: LeafNode if leaf.output.exists(_.name.startsWith("l_")) =>
      SampledRelation(0, numBatches, leaf)
  }
}

case class ResamplePlaceholder(numWays: Int, child: SparkPlan) extends UnaryNode {
  override def output = child.output
  override def execute() = throw new TreeNodeException(this, s"No function to execute plan. type: ${this.nodeName}")
}

/**
 * Insert resampling operator at the optimal place.
 */
object Resample extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    // Insert ResamplePlaceholder
    case sample: SampledRelation => ResamplePlaceholder(1, sample)
    // Optimize by pushing ResamplePlaceholder above Filter, Project, Exchange and Join (in some cases)
    // Note: This optimization is valid because ResamplePlaceholder is only used for tuples from base relations.
    //  Uncertain attributes are generated by aggregates which have to go through join in order to be combined,
    //  and we will merge the bootstrap-counts columns into one immediately after joins.
    case Filter(condition, ResamplePlaceholder(n, child)) => ResamplePlaceholder(n, Filter(condition, child))
    case Project(projectList, ResamplePlaceholder(n, child)) => ResamplePlaceholder(n, Project(projectList, child))
    case Exchange(newPartitioning, ResamplePlaceholder(n, child)) =>
      ResamplePlaceholder(n, Exchange(newPartitioning, child))
    case BroadcastHashJoin(leftKeys, rightKeys, buildSide, ResamplePlaceholder(m, left), ResamplePlaceholder(n, right)) =>
      ResamplePlaceholder(m + n, BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case BroadcastHashJoin(leftKeys, rightKeys, buildSide, ResamplePlaceholder(n, left), right) =>
      ResamplePlaceholder(n, BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, ResamplePlaceholder(n, right)) =>
      ResamplePlaceholder(n, BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, buildSide, ResamplePlaceholder(m, left), ResamplePlaceholder(n, right)) =>
      ResamplePlaceholder(m + n, ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, buildSide, ResamplePlaceholder(n, left), right) =>
      ResamplePlaceholder(n, ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, ResamplePlaceholder(n, right)) =>
      ResamplePlaceholder(n, ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right))
    case LeftSemiJoinHash(leftKeys, rightKeys, ResamplePlaceholder(n, left), right) =>
      ResamplePlaceholder(n, LeftSemiJoinHash(leftKeys, rightKeys, left, right))
  }
}

/**
 * Replace the resampling operator placeholder with its actual implemnetation.
 */
object ImplementResample extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transform {
    case ResamplePlaceholder(n, Project(projectList, child)) =>
      val btCnt = TaggedAlias(Bootstrap(n), Seq.fill(n)(Poisson()).reduce(Multiply), "_btcnt")()
      Project(projectList :+ btCnt, child)

    case ResamplePlaceholder(n, child) =>
      val btCnt = TaggedAlias(Bootstrap(n), Seq.fill(n)(Poisson()).reduce(Multiply), "_btcnt")()
      Project(child.output :+ btCnt, child)
  }
}

case class GuardPlaceholder(sentinel: Expression, trueValue: Expression, falseValue: Expression) extends Expression {
  type EvaluatedType = Any

  def children = sentinel :: trueValue :: falseValue :: Nil

  def nullable = trueValue.nullable || falseValue.nullable

  override lazy val resolved = childrenResolved && trueValue.dataType == falseValue.dataType

  def dataType = {
    if (!resolved) {
      throw new UnresolvedException(
        this,
        s"Can not resolve due to differing types ${trueValue.dataType}, ${falseValue.dataType}")
    }
    trueValue.dataType
  }

  override def eval(input: Row): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def toString = s"if ($sentinel > 0) $trueValue else $falseValue"
}

case class Sum01Placeholder(child: Expression) extends PartialAggregate with trees.UnaryNode[Expression] {

  override def nullable = false

  override def dataType = ByteType

  override def toString = s"SUM01($child)"

  override def asPartial: SplitEvaluation = ???

  override def newInstance() =
    throw new TreeNodeException(this, s"No function to instantiate expression. type: ${this.nodeName}")
}

/**
 * Propagate reampling through the plan, e.g.
 * 1. Propagate bootstrap-counts
 * 2. Aggregates should take in account of bootstrap-counts
 * 3. Join should multiply bootstrap-counts columns
 * (4. Filter should update bootstrap-counts NOTE: handled after lazy-evaluates are identified)
 */
object PropagateBootstrap extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case q: SparkPlan =>
      val withNewLeaves: SparkPlan = q transformExpressionsUp {
        case attr: Attribute => q.inputSet.find(_.exprId == attr.exprId).getOrElse(attr)
      }

      val transformed = withNewLeaves match {
        case project@Project(projectList, child) =>
          getBtCnt(child.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(se1), _, _, _, _)) =>
              var seenBtCnt = false
              val updated = projectList map {
                case TaggedAlias(Bootstrap(se2), expr, name) =>
                  seenBtCnt = true
                  TaggedAlias(Bootstrap(se2 + se1), Multiply(expr, btCnt), name)()
                case alias@Alias(expr, name) =>
                  val (left, right) = widenTypes(expr, Literal(null))
                  Alias(GuardPlaceholder(btCnt, left, right), name)(alias.exprId, alias.qualifiers)
                case expr => expr
              }
              val newProjectList = if (seenBtCnt) updated else updated :+ btCnt
              Project(newProjectList, child)
            case _ => project
          }

        case aggregate@Aggregate(partial, groupings, aggrs, child) =>
          getBtCnt(child.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(se), _, _, _, _)) =>
              val newAggrs = aggrs.map {
                _.transformChildrenUp {
                  case Sum(expr) =>
                    val (left1, right1) = widenTypes(expr, btCnt)
                    val (left2, right2) = widenTypes(Multiply(left1, right1), Literal(0L))
                    if (se != 0) {
                      val (left3, right3) =
                        widenTypes(Sum(GuardPlaceholder(btCnt, left2, right2)), ScaleFactor(1.0, se))
                      Multiply(left3, right3)
                    } else {
                      Sum(GuardPlaceholder(btCnt, left2, right2))
                    }

                  case Count(expr) =>
                    val (left1, right1) = widenTypes(btCnt, Literal(0L))
                    if (se != 0) {
                      val (left2, right2) =
                        widenTypes(Sum(If(IsNotNull(expr), left1, right1)), ScaleFactor(1.0, se))
                      Multiply(left2, right2)
                    } else {
                      Sum(If(IsNotNull(expr), left1, right1))
                    }

                  case _: AggregateExpression => ??? // There should not be any Average
                }
              } :+ TaggedAlias(Bootstrap(0), Sum01Placeholder(btCnt), "_btcnt")()
              Aggregate(partial, groupings, newAggrs, child)
            case _ => aggregate
          }

        case join: BroadcastHashJoin =>
          // Should not optimize Project(Project(join)).
          // For reasons, see resample-push-up optimization.
          postprocess(join)

        case join: ShuffledHashJoin =>
          // Should not optimize Project(Project(join)).
          // For reasons, see resample-push-up optimization.
          postprocess(join)

        case join@LeftSemiJoinHash(leftKeys, rightKeys, left, right) =>
          // Distinct the right branch
          val newJoin = getBtCnt(right.output) match {
            case Some(btCnt@TaggedAttribute(Bootstrap(se), _, _, _, _)) if se != 0 =>
              val newRight = Aggregate(partial = false, rightKeys,
                convertToNamed(rightKeys) :+ TaggedAlias(Bootstrap(0), Sum01Placeholder(btCnt), "_btcnt")(), right)
              LeftSemiJoinHash(leftKeys, rightKeys, left, newRight)
            case _ => join
          }
          // Should not optimize Project(Project(join)).
          // For reasons, see resample-push-up optimization.
          postprocess(newJoin)

        case other => other
      }

      transformed transformExpressionsUp {
        case Cast(expr, dataType) if expr.dataType == dataType => expr // Simplify cast
      }
  }

  def postprocess(plan: SparkPlan): SparkPlan =
    plan.output.partition {
      case TaggedAttribute(Bootstrap(_), _, _, _, _) => true
      case _ => false
    } match {
      case (Seq(a@TaggedAttribute(Bootstrap(x), _, _, _, _), b@TaggedAttribute(Bootstrap(y), _, _, _, _)), others) =>
        Project(others :+ TaggedAlias(Bootstrap(x + y), Multiply(a, b), "_btcnt")(), plan)
      case _ => plan
    }
}

case class FilterProjectPlaceholder(
    projectList: Seq[NamedExpression], child: SparkPlan)(
    val condition: Expression, val flag: Expression)
  extends UnaryNode {
  override def output = projectList.map(_.toAttribute)

  override def execute() = throw new TreeNodeException(this, s"No function to execute plan. type: ${this.nodeName}")

  override def simpleString = super.simpleString + s", [$condition], [$flag]"

  override protected final def otherCopyArgs = condition :: flag :: Nil
}

/**
 * Insert uncertain attributes and flag.
 * Remark:
 *  1. Uncertain attributes are inferred based on the assumption of no partitioning in the input relations.
 *  2. Uncertainty flags are inserted to optimize caching.
 *  3. Any case where the bootstrap-counts may change is flagged uncertain.
 */
object IdentifyUncertainTuples extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case q: SparkPlan =>
      val withNewLeaves: SparkPlan = q transformExpressionsUp {
        case attr: Attribute => q.inputSet.find(_.exprId == attr.exprId).getOrElse(attr)
      }

      withNewLeaves match {
        case filter@Filter(condition, child) if containsUncertain(condition.references) =>
          // The bootstrap-counts column does not need to be lazy,
          // because lazy is for re-evaluation,
          // but if bootstrap-counts may change,
          // it will be marked "uncertain" starting from this Project node
          val updated = child.output.map {
            case btCnt@TaggedAttribute(tag: Bootstrap, _, _, _, _) =>
              TaggedAlias(tag,
                If(And(GreaterThan(btCnt, Literal(0.toByte)), condition), btCnt, Literal(0.toByte)), "_btcnt")(
                  btCnt.exprId, btCnt.qualifiers)
            case attr => attr
          }
          val output = updated.map(_.toAttribute)
          val btCnt = getBtCnt(output).get
          val (projectList, flagExpr) = getFlag(output) match {
            case Some(flag) => (updated, And(flag, ForAll(btCnt)))
            case None => (updated :+ TaggedAlias(Flag, Literal(true), "_flag")(), ForAll(btCnt))
          }
          // we check if the input/output flags are different using ==
          FilterProjectPlaceholder(projectList, child)(Exists(btCnt), flagExpr)

        case project@Project(projectList, child) =>
          val newProjectList = projectList.map {
            case alias@Alias(expr, name) if containsUncertain(alias.references) =>
              TaggedAlias(Uncertain, expr, name)(alias.exprId, alias.qualifiers)
            case other => other
          } ++ getFlag(child.output)
          Project(newProjectList, child)

        case aggregate@Aggregate(partial, groupings, aggrs, child) =>
          require(groupings.forall(key => !containsUncertain(key.references)),
            "group-by keys cannot be uncertain columns")
          if (partial) {
            val newAggrs = aggrs.map {
              case alias@Alias(expr, name) if containsUncertain(alias.references) =>
                TaggedAlias(Uncertain, expr, name)(alias.exprId, alias.qualifiers)
              case expr => expr
            }
            getFlag(child.output) match {
              case Some(flag) => Aggregate(partial, groupings :+ flag, newAggrs :+ flag, child)
              case None => Aggregate(partial, groupings, newAggrs, child)
            }
          } else {
            getBtCnt(child.output) match {
              case Some(btCnt) =>
                val newAggrs = aggrs.map {
                  case alias@Alias(expr, name) if isAggregate(expr) =>
                    TaggedAlias(Uncertain, expr, name)(alias.exprId, alias.qualifiers)
                  case expr => expr
                } :+ {
                  getFlag(child.output) match {
                    case Some(flag) =>
                      TaggedAlias(Flag, ForAll(Sum01Placeholder(If(flag, btCnt, Literal(0.toByte)))), "_flag")()
                    case _ =>
                      TaggedAlias(Flag, ForAll(Sum01Placeholder(btCnt)), "_flag")()
                  }
                }
                Aggregate(partial, groupings, newAggrs, child)
              case _ => aggregate
            }
          }

        case join@BroadcastHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
          // TODO: move uncertain join keys to a filter
          require(leftKeys.forall(key => !containsUncertain(key.references)),
            "join keys cannot be uncertain columns")
          require(rightKeys.forall(key => !containsUncertain(key.references)),
            "join keys cannot be uncertain columns")
          postprocess(join)

        case join@ShuffledHashJoin(leftKeys, rightKeys, buildSide, left, right) =>
          require(leftKeys.forall(key => !containsUncertain(key.references)),
            "join keys cannot be uncertain columns")
          require(rightKeys.forall(key => !containsUncertain(key.references)),
            "join keys cannot be uncertain columns")
          postprocess(join)

        case join@LeftSemiJoinHash(leftKeys, rightKeys, left, right) =>
          require(leftKeys.forall(key => !containsUncertain(key.references)),
            "join keys cannot be uncertain columns")
          require(rightKeys.forall(key => !containsUncertain(key.references)),
            "join keys cannot be uncertain columns")
          postprocess(join)

        case other => other
      }
  }

  def isAggregate(expr: Expression): Boolean = expr match {
    case _: AggregateExpression => true
    case _ => expr.children.exists(isAggregate)
  }

  def postprocess(plan: SparkPlan): SparkPlan =
    plan.output.partition {
      case TaggedAttribute(Flag, _, _, _, _) => true
      case _ => false
    } match {
      case (Seq(f1@TaggedAttribute(Flag, _, _, _, _), f2@TaggedAttribute(Flag, _, _, _, _)), others) =>
        Project(others :+ TaggedAlias(Flag, Add(f1, f2), "_flag")(), plan)
      case _ => plan
    }
}

object PruneFilterProjectColumns extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transform {
    case q: SparkPlan => q.mapChildren {
      case fp@FilterProjectPlaceholder(projectList, child) =>
        val references = q.references.map(_.exprId).toSet
        FilterProjectPlaceholder(projectList.filter(e => references.contains(e.exprId)), child)(fp.condition, fp.flag)
      case other => other
    }
  }
}

case class VectorizePlaceholder(child: Expression) extends UnaryExpression {
  type EvaluatedType = Any

  def nullable = child.nullable
  override def foldable = child.foldable
  def dataType: DataType = ArrayType(child.dataType, child.nullable)

  def eval(input: Row): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def toString = s"Vec($child)"
}

case class DevectorizePlaceholder(child: Expression, dataType: DataType, nullable: Boolean) extends UnaryExpression {
  type EvaluatedType = Any

  def eval(input: Row): Any =
    throw new TreeNodeException(this, s"No function to evaluate expression. type: ${this.nodeName}")

  override def toString = s"Devec($child)"
}

/**
 * Vectorize the whole plan so that all bootstraps are done in one pass.
 * NOTE: we need to propagate all the attributes again because we changed the output attributes of operators.
 */
object ConsolidateBootstrap extends Rule[SparkPlan] {
  import scala.collection.Set

  override def apply(plan: SparkPlan): SparkPlan = {
    val vectors = new mutable.HashSet[ExprId]()

    def markVector(named: NamedExpression): Unit = {
      def ofTypeVector(expr: Expression): Boolean = expr match {
        case TaggedAlias(Bootstrap(_), _, _) => true
        case attr: Attribute => vectors.contains(attr.exprId)
        case _: Exists | _: ForAll => false
        case _ => expr.children.exists(ofTypeVector)
      }

      if (ofTypeVector(named)) vectors += named.exprId
    }

    plan.transformUp {
      case aggregate@Aggregate(partial, groupings, aggrs, child) =>
        aggrs.foreach(markVector)
        Aggregate(partial, groupings, aggrs.map(checkedVectorize(_, vectors)), child)

      case fp@FilterProjectPlaceholder(projectList, child) =>
        projectList.foreach(markVector)
        FilterProjectPlaceholder(projectList.map(checkedVectorize(_, vectors)), child)(fp.condition, fp.flag)

      case project@Project(projectList, child) =>
        projectList.foreach(markVector)
        Project(projectList.map(checkedVectorize(_, vectors)), child)
    }.transformUp { // Propagate attributes with updated types.
      case q: SparkPlan =>
        q.transformExpressions {
          case attr: Attribute => q.inputSet.find(_.exprId ==  attr.exprId).getOrElse(attr)
        }
    }
  }

  def checkedVectorize(expr: NamedExpression, vectors: Set[ExprId]): NamedExpression =
    expr match {
      case _ if expr.references.exists(a => vectors.contains(a.exprId)) => expr.mapChildren(vectorize(_, vectors))
      case TaggedAlias(Bootstrap(_), _, _) => expr.mapChildren(VectorizePlaceholder)
      case _ => expr
    }

  def vectorize(expr: Expression, vectors: Set[ExprId]): Expression = expr match {
    case attr: Attribute => attr
    case Exists(child) => Exists(vectorize(child, vectors))
    case ForAll(child) => ForAll(vectorize(child, vectors))
    case Sum(child) => VectorSum(vectorize(child, vectors))
    case Sum01Placeholder(child) => VectorSum01(vectorize(child, vectors))
    case _: AggregateExpression => ???
    case _ =>
      val visited = new mutable.HashSet[ExprId]()
      val devectorized = expr.transformUp {
        case aggregate: AggregateExpression =>
          visited ++= aggregate.references.map(_.exprId)
          DevectorizePlaceholder(vectorize(aggregate, vectors), aggregate.dataType, aggregate.nullable)
      }.transformUp {
        case attr: Attribute if !visited.contains(attr.exprId) && vectors.contains(attr.exprId) =>
          DevectorizePlaceholder(attr, attr.dataType, attr.nullable)
      }
      VectorizePlaceholder(devectorized)
  }
}

/**
 * Insert lazy evaluates, and prepare for embedding lineage into plan.
 * REMARK:
 *  1. lazy evaluates are only necessary for attributes
 *    whose value can change and will be accessed later in the plan, including be output to users.
 *  2. An attribute may change because it is affected by a running value.
 *    A *running* value is flagged by bootstrap-counts.
 *    Uncertain tuples are always subsumed by tuples with bootstrap-counts,
 *    and thus do not need to be considered for now.
 *  3. we need to propagate all the attributes again because we changed the output attributes of operators.
 */
object IdentifyLazyEvaluates extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case q: SparkPlan =>
      val withNewLeaves: SparkPlan = q transformExpressionsUp {
        case attr: Attribute => q.inputSet.find(_.exprId == attr.exprId).getOrElse(attr)
      }

      withNewLeaves match {
        case aggregate@Aggregate(partial, groupings, aggrs, child) if !partial =>
          require(groupings.forall(key => !containsLazyEvaluates(key.references)),
            "group-by keys cannot be uncertain columns")
          // An partial aggregate does not need to be converted to lazy evaluates,
          // even if its input has lazy evaluates,
          // because its output won't be reused
          val newAggrs = aggrs.map {
            case alias@TaggedAlias(Uncertain, expr, name) =>
              TaggedAlias(LazyAggregate(convertToNamed(groupings), alias.dataType, alias.nullable), expr, name)(
                alias.exprId, alias.qualifiers)
            case expr => expr
          }
          Aggregate(partial, groupings, newAggrs, child)

        case project: Project =>
          project transformExpressionsUp {
            case alias@TaggedAlias(Uncertain, child, name) if containsLazyEvaluates(alias.references) =>
              TaggedAlias(LazyAlias(child), child, name)(alias.exprId, alias.qualifiers)
          }

        case fp: FilterProjectPlaceholder =>
          fp transformExpressionsUp {
            case alias@TaggedAlias(Uncertain, child, name) if containsLazyEvaluates(alias.references) =>
              TaggedAlias(LazyAlias(child), child, name)(alias.exprId, alias.qualifiers)
          }

        case other => other
      }
  }
}

/**
 * Embed lineage into operators.
 */
object EmbedLineage extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case Aggregate(partial, grouping, aggregations, child) =>
      Aggregate(partial, grouping, aggregations ++ lineageNeedToPropagate(aggregations), child)

    case Project(projectList, child) =>
      Project(projectList ++ lineageNeedToPropagate(projectList), child)

    case fp@FilterProjectPlaceholder(projectList, child) =>
      FilterProjectPlaceholder(projectList ++ lineageNeedToPropagate(projectList), child)(fp.condition, fp.flag)
  }

  // Filter out bootstrap-counts, it will be replaced with the bootstrap-counts in the current row.
  // This is a safe optimization to avoid propagating bootstrap-counts,
  // as the bootstrap-counts column is always monotonic in the AND lattice.
  def lineageNeedToPropagate(exprs: Seq[NamedExpression]): Seq[NamedExpression] =
    exprs.collect {
      case TaggedAlias(gen: GenerateLazyEvaluate, _, _) => gen.lineage
      case TaggedAttribute(lazyEval: LazyEvaluate, _, _, _, _) => lazyEval.lineage
    }.flatten.filter {
      case TaggedAttribute(Bootstrap(_), _, _, _, _) => false
      case _ => true
    }.distinct.diff(exprs)
}

/**
 * Replace the guard expression placeholder with its actual implementation.
 */
object ImplementGuard extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformAllExpressions {
    case GuardPlaceholder(sentinel, trueVal, falseVal) =>
      If(GreaterThan(sentinel, Literal(0.toByte)), trueVal, falseVal)
  }
}

/**
 * Replace the vectorize expression placeholder with its actual implementation.
 */
object ImplementVectorize extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformAllExpressions {
    case vec@VectorizePlaceholder(child) =>
      val vectors = new ArrayBuffer[Expression]()
      val scalars = new ArrayBuffer[Expression]()

      val vectorRemoved = child.transformDown {
        case DevectorizePlaceholder(expr, dataType, nullable) =>
          var idx = vectors.indexOf(expr)
          if (idx == -1) {
            idx = vectors.length
            vectors += expr
          }
          BoundReference(idx, dataType, nullable)
      }

      // Mark scalar nodes
      val computeOnce = new ArrayBuffer[Expression]
      vectorRemoved.transformUp {
        case bound: BoundReference =>
          bound
        case poisson: Poisson =>
          // hack: eventually we should separate deterministic/random expressions
          poisson
        case expr if expr.children.forall(e => computeOnce.exists(_.fastEquals(e))) =>
          computeOnce += expr
          expr
      }

      val eval = vectorRemoved.transformDown {
        case expr if computeOnce.exists(_.fastEquals(expr)) =>
          var idx = scalars.indexOf(expr)
          if (idx == -1) {
            idx = scalars.length
            scalars += expr
          }
          BoundReference(vectors.length + idx, expr.dataType, expr.nullable)
      }

      vec.dataType match {
        case ArrayType(dataType, containsNull) =>
          dataType match {
            case StringType
                 | LongType | IntegerType | ShortType | ByteType
                 | DoubleType | FloatType
                 | BooleanType =>
              ImmutableVectorize(eval, vectors, scalars)
            case _ =>
              ??? // TODO: MutableVectorize
          }
      }
  }
}



/**
 * Remark:
 * 1. Take broadcast implementation out of aggregate, that can be implemented as a separate operator,
 *  and optimized by considering the plan as a whole (e.g., later broadcast join).
 * 2. Only aggregates that output lazy evaluates need to be broadcast.
 * 3. We have already assumed no partitioning of input relations.
 * 4. Caching can be categorized into 6 classes according to
 *  (1) whether the input tuples are uncertain, lazy or normal attributes
 *  (2) whether there is a flag column
 *
 *  uncertain --> no caching
 *  lazy --> per tuple caching
 *  normal --> partial result caching (TODO: currently use per tuple caching)
 *
 *  if flag exists, only those tuples with flag=true will be cached or merged into the cached partial result
 */
object ImplementAggregate extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case aggregate@Aggregate(partial, groupings, aggrs, child) =>
      val references = aggrs.flatMap(_.references)
      if (containsUncertain(references)) {
        aggregate
      } else if (containsLazyEvaluates(references)) {
        val lazyEvals = child.output.collect {
          case TaggedAttribute(eval: LazyEvaluate, _, _, _, _) => eval.lazyEval
          case attr => attr
        }
        OnlineAggregate(getFlag(child.output), Some(lazyEvals), partial, groupings, aggrs, child)()
      } else if (containsLazyEvaluates(aggregate.output)) {
        OnlineAggregate(getFlag(child.output), None, partial, groupings, aggrs, child)()
      } else {
        aggregate
      }
  }
}

object CleanupAnalysisExpressions extends Rule[SparkPlan] {
  override def apply(plan: SparkPlan): SparkPlan = plan transformUp {
    case q: SparkPlan =>
      q transformExpressionsUp {
        case attr@TaggedAttribute(_, name, dataType, nullable, metadata) =>
          AttributeReference(name, dataType, nullable, metadata)(attr.exprId, attr.qualifiers)
        case alias@TaggedAlias(_, child, name) =>
          Alias(child, name)(alias.exprId, alias.qualifiers)
      }
  }
}

object OnlinePlannerUtil {
  /**
   * Get the first BootstrapCounts if any of this plan.
   */
  def getBtCnt(attributes: Seq[Attribute]): Option[TaggedAttribute] =
    attributes.collectFirst { case btCnt@TaggedAttribute(Bootstrap(_), _, _, _, _) => btCnt}

  /**
   * Get the first flag if any of this plan
   */
  def getFlag(attributes: Seq[Attribute]): Option[TaggedAttribute] =
    attributes.collectFirst { case flag@TaggedAttribute(Flag, _, _, _, _) => flag}

  /**
   * Whether attributes contain uncertain ones.
   */
  def containsUncertain(attributes: Traversable[Attribute]) = attributes.exists {
    case TaggedAttribute(Uncertain, _, _, _, _) => true
    case _ => false
  }

  /**
   * Whether attributes contain lazy evalutes.
   */
  def containsLazyEvaluates(attributes: Traversable[Attribute]) = attributes.exists {
    case TaggedAttribute(LazyAttribute(_), _, _, _, _) => true
    case _ => false
  }

  def widenTypes(left: Expression, right: Expression): (Expression, Expression) =
    if (left.dataType != right.dataType)
      HiveTypeCoercion.findTightestCommonType(left.dataType, right.dataType)
        .map { widestType => (castTo(left, widestType), castTo(right, widestType))}
        .getOrElse((left, right))
    else (left, right)

  def castTo(expr: Expression, dataType: DataType) =
    if (expr.dataType != dataType) {
      expr match {
        case _: Literal =>
          val casted = Cast(expr, dataType)
          Literal(casted.eval(null), casted.dataType)
        case _ =>
          Cast(expr, dataType)
      }
    } else expr

  def convertToNamed(exprs: Seq[Expression], name: String = "_key"): Seq[NamedExpression] =
    exprs map {
      case named: NamedExpression => named
      case expr => Alias(expr, name)()
    }

  def getGrowthMode(plan: SparkPlan): GrowthMode = {
    import org.apache.spark.sql.online.Growth._
    plan match {
      case _: LeafNode => GrowthMode(Fixed, Fixed)
      case SampledRelation(_, _, child) =>
        getGrowthMode(child) match {
          case GrowthMode(Fixed, Fixed) => GrowthMode(Fixed, Grow)
          case g => throw new TreeNodeException(plan, s"Imcompatible growth mode $g in $plan.")
        }
      case Exchange(_, child) =>
        getGrowthMode(child) match {
          case GrowthMode(p, n) => GrowthMode(Growth.max(p, n), Fixed)
        }
      case Aggregate(_, _, _, child) =>
        getGrowthMode(child) match {
          case GrowthMode(p, n) => GrowthMode(Growth.min(p, AlmostFixed), n)
        }
      case _: ShuffledHashJoin | _: LeftSemiJoinHash =>
        val modes = plan.children.map(getGrowthMode)
        modes.forall(_.numPartitions == modes(0).numPartitions)
        if (modes.forall(_.numPartitions == modes(0).numPartitions)) {
          GrowthMode(modes.map(_.perPartition).reduce(Growth.max),
            modes.map(_.numPartitions).reduce(Growth.max))
        } else {
          throw new TreeNodeException(plan, s"Imcompatible growth mode ${modes.mkString(", ")} in $plan.")
        }
      case BroadcastHashJoin(_, _, buildSide, left, right) =>
        val (buildMode, streamMode) = buildSide match {
          case BuildLeft => (getGrowthMode(left), getGrowthMode(right))
          case BuildRight => (getGrowthMode(right), getGrowthMode(left))
        }
        streamMode match {
          case GrowthMode(p, n) =>
            GrowthMode(Seq(p, buildMode.perPartition, buildMode.numPartitions).reduce(Growth.max), n)
        }
      case unary: UnaryNode => getGrowthMode(unary.child)
      case _ => ???
    }
  }
}

object Growth extends Enumeration {
  type Growth = Value
  val Fixed, AlmostFixed, Grow = Value

  def min(x: Growth, y: Growth) = if (x < y) x else y

  def max(x: Growth, y: Growth) = if (x < y) y else x
}

case class GrowthMode(perPartition: Growth.Growth, numPartitions: Growth.Growth) {
  require(numPartitions != Growth.AlmostFixed)
}